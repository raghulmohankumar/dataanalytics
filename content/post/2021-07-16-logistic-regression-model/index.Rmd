---
title: Logistic Regression Model
author: Raghul Mohankumar
date: '2021-07-16'
slug: logistic-regression-model
categories:
  - ggplot2
  - caret
  - models
  - Tidyverse
tags: []
---


<style>
body {
text-align: justify}
</style>

## Problem Statement:

The data is related with direct marketing campaigns of a Portuguese banking institution, which is rolling out term deposit for its customers. The marketing campaigns were based on phone calls. Often, more than one contact to the same client was required, in order to access if the product (bank term deposit) would be (or not) subscribed. The classification goal is to predict if the client will subscribe a term deposit (variable y).

## Data:

There are two datasets: 

      1) bank-full.csv with all examples, ordered by date (from May 2008 to November 2010).
      
      2) bank.csv with 10% of the examples (4521), randomly selected from bank-full.csv.
      
The smallest dataset is provided to test more computationally demanding machine learning algorithms (e.g. SVM).

Number of Instances: 45211 for bank-full.csv (4521 for bank.csv)
Number of Attributes: 16 + output attribute

## Data dictionary:

### Variables : Definition: Type and their categories

Each row represents characteristic of a single customer . Many categorical data has been coded to mask the data.

1 - age (numeric)

2 - job : type of job (categorical: “admin.”, “unknown”, “unemployed”, “management”, “housemaid”, “entrepreneur”, “student”, “blue-collar”, “self-employed”, “retired”,“technician”, “services”) 

3 - marital : marital status (categorical: “married”, “divorced”, “single”; note: “divorced” means divorced or widowed)

4 - education (categorical: “unknown”, “secondary”, “primary”, “tertiary”)

5 - default: has credit in default? (binary: “yes”, “no”)

6 - balance: average yearly balance, in euros (numeric)

7 - housing: has housing loan? (binary: “yes”,“no”)

8 - loan: has personal loan? (binary: “yes”,“no”)

Related with the last contact of the current campaign:

9 - contact: contact communication type (categorical: “unknown”, “telephone”, “cellular”)

10 - day: last contact day of the month (numeric))

Direct Marketing Campaign: Details and Phase I Tasks

11 - month: last contact month of year (categorical: “jan”, “feb”, “mar”, . . . , “nov”, “dec”)

12 - duration: last contact duration, in seconds (numeric)

other attributes: 13 - campaign: number of contacts performed during this campaign and for this client (numeric, includes last contact)

14 - pdays: number of days that passed by after the client was last contacted from a previous campaign (numeric, -1 means client was not previously contacted) 

15 - previous: number of contacts performed before this campaign and for this client (numeric)

16 - poutcome: outcome of the previous marketing campaign (categorical: “unknown”, “other”, “failure”, “success”)

Output variable (desired target):

17 - y - has the client subscribed a term deposit? (binary: “yes”,“no”)



## Methodology:

We will build a Logistic regression model to predict the response variable “y” (whether the client subscribed to a term deposit or No.)

Step 1: Imputing NA values in the datasets.

Step 2:Data Preparation: Grouping similar category variables and making dummies.

Step 3: Model Building( LOGISTIC REGRESSION )

Step 4. Finding Cutoff value and Perfomance measurements of the model.(Sensitivity, Specificity, Accuracy)

Step 5.Predict the final output on test dataset.(whether the client subscribe or no to term deposit)

Step 6:Creating confusion matrix and finding how good the model is.


## Load Packages
```{r warning=FALSE,echo=FALSE,message=FALSE}
library(dplyr)
library(car)
library(ggplot2)
library(pROC)
```

## Import Datasets : Bank data
```{r warning=FALSE,echo=FALSE,message=FALSE}
test=read.csv("bank.csv",sep = ";" ) 
train=read.csv("bank-full.csv", sep = ";") 
```

## Step 1: Adding NA values in the datasets

```{r warning=FALSE,echo=FALSE,message=FALSE}
apply(train,2,function(x)sum(is.na(x)))
apply(test,2,function(x)sum(is.na(x)))
```
There are no NA values in train and test datasets.

## Step 2:Data Preparation

Combining both train and test datasets prior to data preparation.

```{r warning=FALSE,echo=FALSE,message=FALSE}
test$y=NA
train$data='train'
test$data='test'
all_data=rbind(train,test)
apply(all_data,2,function(x)sum(is.na(x)))

#droping balance column as it is an undocumented column
all_data$balance <- NULL

glimpse(all_data)
```

## Creating dummy variables by combining similar categories

### Dummy for Job

```{r warning=FALSE,echo=FALSE,message=FALSE}
t=table(all_data$job)
sort(t)
final=round(prop.table(table(all_data$job,all_data$y),1)*100,1)
s=addmargins(final,2) #add margin across Y
sort(s[,1])
all_data=all_data %>% 
  mutate(job_1=as.numeric(job %in% c("self-employed","unknown","technician")), 
         job_2=as.numeric(job %in% c("services","housemaid","entrepreneur")),
         job_3=as.numeric(job %in% c("management","admin")),
         job_4=as.numeric(job=="student"),
         job_5=as.numeric(job=="retired"),
         job_6=as.numeric(job=="unemployed")) %>% 
  select(-job)
```

### Dummy for marital

```{r warning=FALSE,echo=FALSE,message=FALSE}
t=table(all_data$marital)
sort(t)
all_data=all_data %>% 
  mutate(divorced=as.numeric(marital %in% c("divorced")),
         single=as.numeric(marital %in% c("single"))
         ) %>% 
  select(-marital)
```

### Dummy for education

```{r warning=FALSE,echo=FALSE,message=FALSE}
t=table(all_data$education)
sort(t)
all_data=all_data %>% 
  mutate(edu_primary=as.numeric(education %in% c("primary")),
         edu_sec=as.numeric(education %in% c("secondary")),
         edu_tert=as.numeric(education %in% c("tertiary"))
  ) %>% 
  select(-education)
```

### Dummy for default

```{r warning=FALSE,echo=FALSE,message=FALSE}
table(all_data$default)
all_data$default=as.numeric(all_data$default=="yes")
```

### Dummy for housing

```{r warning=FALSE,echo=FALSE,message=FALSE}
table(all_data$housing)
all_data$housing=as.numeric(all_data$housing=="yes")
```

### Dummy for loan

```{r warning=FALSE,echo=FALSE,message=FALSE}
table(all_data$loan)
all_data$loan=as.numeric(all_data$loan=="yes")
```

### Dummy for contact

```{r warning=FALSE,echo=FALSE,message=FALSE}
t=table(all_data$contact)
sort(t)
all_data=all_data %>% 
  mutate(co_cellular=as.numeric(contact %in% c("cellular")),
         co_tel=as.numeric(contact %in% c("telephone"))
  ) %>% 
  select(-contact)
```

### Dummy for month

```{r warning=FALSE,echo=FALSE,message=FALSE}
t=table(all_data$month)

#Percentage Conversion across months.
finalmnth=round(prop.table(table(all_data$month,all_data$y),1)*100,1)
sss=addmargins(finalmnth,2) #adding margin across Y
sort(sss[,1])

#May is considered as base var
all_data=all_data %>% 
  mutate(month_1=as.numeric(month %in% c("aug","jun","nov","jan","jul")), 
         month_2=as.numeric(month %in% c("dec","sep")),
         month_3=as.numeric(month=="mar"),
         month_4=as.numeric(month=="oct"),
         month_5=as.numeric(month=="apr"),
         month_6=as.numeric(month=="feb")) %>% 
select(-month)
```

### Dummy for outcome

```{r warning=FALSE,echo=FALSE,message=FALSE}
t=table(all_data$poutcome)
sort(t)
all_data=all_data %>% 
  mutate(poc_success=as.numeric(poutcome %in% c("success")),
         poc_failure=as.numeric(poutcome %in% c("failure")),
         poc_other=as.numeric(poutcome %in% c("other"))
         )%>% 
           select(-poutcome)
```
### After Data Preparation

```{r warning=FALSE,echo=FALSE,message=FALSE}
table(all_data$y)
table(train$y)
all_data$y=as.numeric(all_data$y=="yes")
table(all_data$y)
glimpse(all_data)
```

### After Separating test and train

```{r warning=FALSE,echo=FALSE,message=FALSE}
train=all_data %>% 
  filter(data=='train') %>% 
  select(-data) 
glimpse(train)

test=all_data %>% 
  filter(data=='test') %>% 
  select(-data,-y)
glimpse(test)


# Dividing the train dataset in the ratio 75:25

set.seed(5)
s=sample(1:nrow(train),0.75*nrow(train))
train_75=train[s,]
test_25=train[-s,]
```

## Step 3: Model Building

We will use train for logistic regression model building and use train_25 to test the performance of the model thus built.
Lets build logistic regression model on train dataset.

```{r warning=FALSE,echo=FALSE,message=FALSE}
for_vif=lm(y~.,data=train)
summary(for_vif)
```

### Fine tuning for multi-collinearity

```{r warning=FALSE,echo=FALSE,message=FALSE}
# Removing variables whose  variance inflation factor >5
t=vif(for_vif)
sort(t,decreasing = T)[1:5]

# Removing edu_sec

for_vif=lm(y~.-edu_sec,data=train)
t=vif(for_vif)
sort(t,decreasing = T)[1:5]

fit_train=train %>% 
  select(-edu_sec)

fit=glm(y~.,family = "binomial",data=fit_train) 
summary(fit)

# Removing all variables whose p value is >0.05 using step function.

fit=step(fit)

# Checking the remaining significant variables

names(fit$coefficients)
```


### Building final logistic model on significant variables

```{r warning=FALSE,echo=FALSE,message=FALSE}
fit_final=glm(y~housing+loan+duration+campaign+previous+job_2+job_4+job_5+divorced+single+edu_primary+edu_tert+co_cellular+co_tel+month_2+month_3+month_4+month_5+month_6+poc_success+poc_failure+poc_other,data=fit_train,family="binomial")
summary(fit_final)
names(fit_final$coefficients)
```

### Predict Scores and visualisation

```{r warning=FALSE,echo=FALSE,message=FALSE}
train$score=predict(fit_final,newdata = train,type="response")
ggplot(train,aes(y=y,x=score,color=factor(y)))+geom_point()+geom_jitter()
```

## Step 4. Finding Cutoff value and Perfomance measurements of the model.

```{r warning=FALSE,echo=FALSE,message=FALSE}

# Finding cutoff based on these probability scores
cutoff_data=data.frame(cutoff=0,TP=0,FP=0,FN=0,TN=0)
cutoffs=seq(0,1,length=100)

for (i in cutoffs){
  predicted=as.numeric(train$score>i)
  
  TP=sum(predicted==1 & train$y==1)
  FP=sum(predicted==1 & train$y==0)
  FN=sum(predicted==0 & train$y==1)
  TN=sum(predicted==0 & train$y==0)
  cutoff_data=rbind(cutoff_data,c(i,TP,FP,FN,TN))
}
## Removing the dummy data cotaining top row in data frame cutoff_data
cutoff_data=cutoff_data[-1,]

## Calculating the performance measures:sensitivity,specificity,accuracy, KS and precision

cutoff_data=cutoff_data %>%
  mutate(P=FN+TP,N=TN+FP, #total positives and negatives
         Sn=TP/P, #sensitivity
         Sp=TN/N, #specificity
         KS=abs((TP/P)-(FP/N)),
         Accuracy=(TP+TN)/(P+N),
         Lift=(TP/P)/((TP+FP)/(P+N)),
         Precision=TP/(TP+FP),
         Recall=TP/P
  ) %>% 
  select(-P,-N)

# Finding cutoff value based on ks MAXIMUM

KS_cutoff=cutoff_data$cutoff[which.max(cutoff_data$KS)]
KS_cutoff
```

Therefore, 0.1010101 is the cutoff value by ks max method.


## Step 5.Predict if the client subscribe or no to term deposit

```{r warning=FALSE,echo=FALSE,message=FALSE}
# Predicting test scores

test$score=predict(fit_final,newdata =test,type = "response")

# Predicting if the client has subscribed or not

test$left=as.numeric(test$score>KS_cutoff)

# Final Predection

test$leftfinal=factor(test$left,levels = c(0,1),labels=c("no","yes"))
table(test$leftfinal)
```
Thus, 1213 customers out of 4521 subscribe to term deposit according to the model.


## Step 6: Creating confusion matrix to find the goodness of our model

```{r warning=FALSE,echo=FALSE,message=FALSE}
test_25$score=predict(fit_final,newdata =test_25,type = "response")
n1= table(test_25$y,as.numeric(test_25$score>KS_cutoff))
n2= table(test_25$y)
bv=as.numeric(as.matrix(n1))
bt=as.numeric(as.matrix(n2))
TP=bv[1]
TN=bv[4]
P=bt[1]
N=bt[2]
# Accuracy=(TP+TN)/(P+N):
a= (TP+TN)/(P+N)
cat("The Accuracy of the model is ",a,"\n")
cat("The Error of the model is ",1-a, "(according to ks method)")

# Plotting the ROC curve
roccurve=roc(test_25$y,test_25$score) 
plot(roccurve)
auc(roccurve)
```
## Conclusion:
Ultimately, the target number of customers to be focused upon for term deposits by the bank are predicted successfully using logistic regression model with an accuracy of 81.35% using KS method. It predicts that 1213 customers out of 4521 subscribe to term deposit according to the model.The area under the ROC curve is 0.92/1, which shows that the model and predictions are very good.